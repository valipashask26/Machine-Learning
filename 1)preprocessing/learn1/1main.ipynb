{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dbee66f0-6c70-452b-b334-5f58711f340d",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b253a190-27ab-4f8d-a625-082e15958185",
   "metadata": {},
   "source": [
    "## importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "71e0acad-9711-4057-9379-543826abe9ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edcf3d2c-622b-415a-a792-18bd76ea897c",
   "metadata": {},
   "source": [
    "## calling the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "532f0fc0-d56d-44f3-87f1-596fd6d2040f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['France' 44.0 72000.0]\n",
      " ['Spain' 27.0 48000.0]\n",
      " ['Germany' 30.0 54000.0]\n",
      " ['Spain' 38.0 61000.0]\n",
      " ['Germany' 40.0 nan]\n",
      " ['France' 35.0 58000.0]\n",
      " ['Spain' nan 52000.0]\n",
      " ['France' 48.0 79000.0]\n",
      " ['Germany' 50.0 83000.0]\n",
      " ['France' 37.0 67000.0]]\n",
      "['No' 'Yes' 'No' 'No' 'Yes' 'Yes' 'No' 'Yes' 'No' 'Yes']\n"
     ]
    }
   ],
   "source": [
    "dataset = pd.read_csv(\"Data.csv\")\n",
    "X = dataset.iloc[:, :-1].values      ##from the dataset, X is ref to feature/independent variables\n",
    "Y = dataset.iloc[:, -1].values       ##from the dataset, Y is ref to dependent/target variables\n",
    "\n",
    "print (X)\n",
    "print (Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6817b33-b275-40e6-a9f7-f1b9ea1c6451",
   "metadata": {},
   "source": [
    "## Filling the missing data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd6a85c3-56b7-498b-9168-e194476970a8",
   "metadata": {},
   "source": [
    "### using mean technique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c893d552-345f-40b8-9831-8376df69f1cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['France' 44.0 72000.0]\n",
      " ['Spain' 27.0 48000.0]\n",
      " ['Germany' 30.0 54000.0]\n",
      " ['Spain' 38.0 61000.0]\n",
      " ['Germany' 40.0 63777.77777777778]\n",
      " ['France' 35.0 58000.0]\n",
      " ['Spain' 38.77777777777778 52000.0]\n",
      " ['France' 48.0 79000.0]\n",
      " ['Germany' 50.0 83000.0]\n",
      " ['France' 37.0 67000.0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "imputer.fit(X[:, 1:3])\n",
    "X[:, 1:3] = imputer.transform(X[:, 1:3])\n",
    "print (X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa0a218-ce39-4209-bced-9ab91c92705f",
   "metadata": {},
   "source": [
    "### using median technique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "910ee50a-9729-4c1e-a114-c110a4e0bc5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['France' 44.0 72000.0]\n",
      " ['Spain' 27.0 48000.0]\n",
      " ['Germany' 30.0 54000.0]\n",
      " ['Spain' 38.0 61000.0]\n",
      " ['Germany' 40.0 63777.77777777778]\n",
      " ['France' 35.0 58000.0]\n",
      " ['Spain' 38.77777777777778 52000.0]\n",
      " ['France' 48.0 79000.0]\n",
      " ['Germany' 50.0 83000.0]\n",
      " ['France' 37.0 67000.0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "imputer = SimpleImputer(missing_values=np.nan, strategy='median')\n",
    "imputer.fit(X[:, 1:3])\n",
    "X[:, 1:3] = imputer.transform(X[:, 1:3])\n",
    "print (X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a0cf1aa-bd03-467b-a3cd-09daf71198f1",
   "metadata": {},
   "source": [
    "### Most Frequent Imputation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc480824-4a6e-4a51-84f8-9684622cd436",
   "metadata": {},
   "source": [
    "For categorical variables, you can fill missing values with the most frequent value in the column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8ba7cf87-091b-407d-afa4-c5303466d0b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['France' 44.0 72000.0]\n",
      " ['Spain' 27.0 48000.0]\n",
      " ['Germany' 30.0 54000.0]\n",
      " ['Spain' 38.0 61000.0]\n",
      " ['Germany' 40.0 48000.0]\n",
      " ['France' 35.0 58000.0]\n",
      " ['Spain' 27.0 52000.0]\n",
      " ['France' 48.0 79000.0]\n",
      " ['Germany' 50.0 83000.0]\n",
      " ['France' 37.0 67000.0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "imputer = SimpleImputer(missing_values=np.nan, strategy='most_frequent') \n",
    "imputer.fit(X[:, 1:3])\n",
    "X[:, 1:3] = imputer.transform(X[:, 1:3])\n",
    "print (X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbbbcf27-853a-4253-9e56-00b37755f38b",
   "metadata": {},
   "source": [
    "### Constant imputation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ccc963-722d-4176-a9d6-e7fa2f484cd7",
   "metadata": {},
   "source": [
    "You can fill missing values with a constant value of your choice. This is useful when missing values have a specific meaning in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "be8340aa-9f31-45f1-b49e-a60107e3908b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0 1.0 0.0 1.0 1.0 0.0 0.0 44.0 72000.0]\n",
      " [1.0 0.0 1.0 0.0 1.0 0.0 1.0 27.0 48000.0]\n",
      " [1.0 0.0 1.0 0.0 0.0 1.0 0.0 30.0 54000.0]\n",
      " [1.0 0.0 1.0 0.0 1.0 0.0 1.0 38.0 61000.0]\n",
      " [1.0 0.0 1.0 0.0 0.0 1.0 0.0 40.0 63777.77777777778]\n",
      " [0.0 1.0 0.0 1.0 1.0 0.0 0.0 35.0 58000.0]\n",
      " [1.0 0.0 1.0 0.0 1.0 0.0 1.0 38.77777777777778 52000.0]\n",
      " [0.0 1.0 0.0 1.0 1.0 0.0 0.0 48.0 79000.0]\n",
      " [1.0 0.0 1.0 0.0 0.0 1.0 0.0 50.0 83000.0]\n",
      " [0.0 1.0 0.0 1.0 1.0 0.0 0.0 37.0 67000.0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "imputer = SimpleImputer(missing_values=np.nan, strategy='constant',fill_value=0)\n",
    "imputer.fit(X[:, 1:3])\n",
    "X[:, 1:3] = imputer.transform(X[:, 1:3])\n",
    "print (X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2514e009-6b9a-4591-aa74-66ad1602fe09",
   "metadata": {},
   "source": [
    "If missing values are too prevalent or cannot be imputed accurately, you may choose to simply drop rows or columns with missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f9be26-5c97-4f84-b664-bba63f236ec6",
   "metadata": {},
   "source": [
    "## Encoding values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "557eabcb-f045-4037-9676-9f1f16a451f8",
   "metadata": {},
   "source": [
    "### encoding the independent values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "648bbf8e-ece6-40d2-8ba7-8230f92865ac",
   "metadata": {},
   "source": [
    "Encoding in the context of machine learning is the process of converting categorical variables into a numerical representation that can be used by machine learning algorithms for analysis and modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d88005-bbb7-4f09-bc93-44230adea0b8",
   "metadata": {},
   "source": [
    "#### One Hot encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f503b1de-6e92-4bab-b5c5-93ad10c0e2b6",
   "metadata": {},
   "source": [
    "Converts categorical variables into binary vectors, where each category becomes a binary feature.\n",
    "\n",
    "Suitable for nominal variables without any inherent order.\n",
    "\n",
    "Helps prevent ordinality assumption by the model.\n",
    "\n",
    "Implemented using libraries like scikit-learn's OneHotEncoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a9d1e3bb-4078-4b5f-9dd0-1c44618fb7b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.0 0.0 0.0 44.0 72000.0]\n",
      " [0.0 0.0 1.0 27.0 48000.0]\n",
      " [0.0 1.0 0.0 30.0 54000.0]\n",
      " [0.0 0.0 1.0 38.0 61000.0]\n",
      " [0.0 1.0 0.0 40.0 63777.77777777778]\n",
      " [1.0 0.0 0.0 35.0 58000.0]\n",
      " [0.0 0.0 1.0 38.77777777777778 52000.0]\n",
      " [1.0 0.0 0.0 48.0 79000.0]\n",
      " [0.0 1.0 0.0 50.0 83000.0]\n",
      " [1.0 0.0 0.0 37.0 67000.0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "ct = ColumnTransformer(transformers=[('encoder',OneHotEncoder(),[0])],remainder='passthrough')\n",
    "X = np.array(ct.fit_transform(X))\n",
    "print (X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc0c4c0-6f7c-4c99-9976-431c5f94fe49",
   "metadata": {},
   "source": [
    "### encoding the dependent values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99cfbefc-377d-4532-82b0-876b7ffc59b1",
   "metadata": {},
   "source": [
    "#### Label Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7998579-8273-47f1-ab66-ca1862c44cb0",
   "metadata": {},
   "source": [
    "Label encoding is a method of converting categorical variables into numerical format by assigning a unique integer label to each category. This encoding preserves the ordinal relationship among categories, making it suitable for categorical variables with a natural order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b7c659ef-1dda-4a59-8939-d750a2ba8e91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 0 0 1 1 0 1 0 1]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "Y = le.fit_transform(Y)\n",
    "print (Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ca78298a-2d08-448b-8373-e57ed821b093",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.0 0.0 0.0 44.0 72000.0 0]\n",
      " [0.0 0.0 1.0 27.0 48000.0 1]\n",
      " [0.0 1.0 0.0 30.0 54000.0 0]\n",
      " [0.0 0.0 1.0 38.0 61000.0 0]\n",
      " [0.0 1.0 0.0 40.0 63777.77777777778 1]\n",
      " [1.0 0.0 0.0 35.0 58000.0 1]\n",
      " [0.0 0.0 1.0 38.77777777777778 52000.0 0]\n",
      " [1.0 0.0 0.0 48.0 79000.0 1]\n",
      " [0.0 1.0 0.0 50.0 83000.0 0]\n",
      " [1.0 0.0 0.0 37.0 67000.0 1]]\n"
     ]
    }
   ],
   "source": [
    "xy_table = np.column_stack((X, Y))   ##prints as full table\n",
    "print (xy_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f3cf8a-503d-4f29-9519-3cdca766da56",
   "metadata": {},
   "source": [
    "## Splitting the data for training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7213b1d1-0ba5-44ce-859f-a1390d08efde",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y,test_size=0.2,random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "18817aa5-d85d-4b57-acdc-d19c2ddba8c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0 0.0 1.0 38.77777777777778 52000.0]\n",
      " [0.0 1.0 0.0 40.0 63777.77777777778]\n",
      " [1.0 0.0 0.0 44.0 72000.0]\n",
      " [0.0 0.0 1.0 38.0 61000.0]\n",
      " [0.0 0.0 1.0 27.0 48000.0]\n",
      " [1.0 0.0 0.0 48.0 79000.0]\n",
      " [0.0 1.0 0.0 50.0 83000.0]\n",
      " [1.0 0.0 0.0 35.0 58000.0]]\n"
     ]
    }
   ],
   "source": [
    "print (X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a63f6504-ab8c-4afd-b912-365c3e98ced8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0 1.0 0.0 30.0 54000.0]\n",
      " [1.0 0.0 0.0 37.0 67000.0]]\n"
     ]
    }
   ],
   "source": [
    "print (X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5723edd3-3af1-448a-9bf3-22aea55618f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 0 0 1 1 0 1]\n"
     ]
    }
   ],
   "source": [
    "print (Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "11aacfd5-8380-4d8a-89f6-645d44d03241",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1]\n"
     ]
    }
   ],
   "source": [
    "print (Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51884690-e3fd-40ad-b7fd-66035e1a1220",
   "metadata": {},
   "source": [
    "## Feature Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d0b6c6c-6043-4fa6-94ed-a719b06e078f",
   "metadata": {},
   "source": [
    "Feature scaling is a preprocessing technique in machine learning that transforms the values of features to a specific range or distribution, such as standardization (Z-score normalization) or normalization (Min-Max scaling), to ensure all features have the same scale and prevent dominance of features with larger magnitudes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca6a52d-17ab-4b96-aa1e-4ca54b15d1de",
   "metadata": {},
   "source": [
    "it tries to put the all values in same scale"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3559d122-f55a-44dd-adb1-a2a815342660",
   "metadata": {},
   "source": [
    "standardisation = x - mean(x) / sd(x)\n",
    "\n",
    "normalization   = x - mean(x) / max(x) - min(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f54112-702b-480a-8889-9a4f25f5183c",
   "metadata": {},
   "source": [
    "normalization will be preferred when we have normal distribution in features\n",
    "\n",
    "standardization will be preferred for all the time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4c15a945-7e67-44ec-98d6-e55bff8e7985",
   "metadata": {},
   "outputs": [],
   "source": [
    "## preferring Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8776e62f-d893-4d3f-ac2c-168538ff5bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scc = StandardScaler()\n",
    "X_train[:, 3:] = scc.fit_transform(X_train[:, 3:])\n",
    "X_test[:, 3:] = scc.transform(X_test[:, 3:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "402bf8f9-ca93-4b0d-9052-b2a1272a46a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0 0.0 1.0 -0.19159184384578545 -1.0781259408412425]\n",
      " [0.0 1.0 0.0 -0.014117293757057777 -0.07013167641635372]\n",
      " [1.0 0.0 0.0 0.566708506533324 0.633562432710455]\n",
      " [0.0 0.0 1.0 -0.30453019390224867 -0.30786617274297867]\n",
      " [0.0 0.0 1.0 -1.9018011447007988 -1.420463615551582]\n",
      " [1.0 0.0 0.0 1.1475343068237058 1.232653363453549]\n",
      " [0.0 1.0 0.0 1.4379472069688968 1.5749910381638885]\n",
      " [1.0 0.0 0.0 -0.7401495441200351 -0.5646194287757332]]\n"
     ]
    }
   ],
   "source": [
    "print (X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e008bc48-f8b4-44cf-9500-a8e913106a4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0 1.0 0.0 -1.4661817944830124 -0.9069571034860727]\n",
      " [1.0 0.0 0.0 -0.44973664397484414 0.2056403393225306]]\n"
     ]
    }
   ],
   "source": [
    "print (X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e99a57-65d5-4cd5-9be5-4526cdc23780",
   "metadata": {},
   "source": [
    "In the given code snippet, `fit_transform` is applied to the training data (`X_train[:, 3:]`), but only `transform` is applied to the test data (`X_test[:, 3:]`). This is because:\n",
    "\n",
    "1. **Fit**: `fit_transform` is used on the training data to compute the mean and standard deviation of each feature in order to standardize the data. The `fit` part calculates the mean and standard deviation based on the training data.\n",
    "\n",
    "2. **Transform**: Once the mean and standard deviation are computed from the training data, `transform` is applied to both the training and test data to standardize them using the same mean and standard deviation learned from the training data. This ensures that the test data is scaled in the same way as the training data.\n",
    "\n",
    "By applying `fit_transform` only to the training data and `transform` to the test data, we prevent data leakage and ensure that the test data is scaled based on the same parameters learned from the training data. This maintains the integrity of the test set and ensures that the model is evaluated on unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ecb08632-fcab-42c5-99fd-54d3ef9c2529",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'from sklearn.preprocessing import MinMaxScaler\\n\\nscaler = MinMaxScaler()\\n\\n\\nX_train[:, 3:] = scaler.fit_transform(X_train[:, 3:])\\n\\n\\n* X_test[:, 3:] = scaler.transform(X_test[:, 3:]) '"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## if we use normalization\n",
    "\n",
    "\"\"\"from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "\n",
    "X_train[:, 3:] = scaler.fit_transform(X_train[:, 3:])\n",
    "\n",
    "\n",
    "* X_test[:, 3:] = scaler.transform(X_test[:, 3:]) \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0462389d-8cb3-4601-a49b-749968a5792a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PY (sk-allenv)",
   "language": "python",
   "name": "sk-allenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
